{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b784f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7317e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfea2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_features(input):\n",
    "    vgg=tf.keras.applications.VGG16()\n",
    "    # Inputs to a layer should be tensors\n",
    "    f2=vgg.get_layer('block2_pool').output\n",
    "    f3=vgg.get_layer('block3_pool').output\n",
    "    f4=vgg.get_layer('block4_pool').output\n",
    "    f5=vgg.get_layer('block5_pool').output\n",
    "    f1=vgg.get_layer('block1_pool').output\n",
    "    f2=vgg.get_layer('block2_pool').output\n",
    "    f3=vgg.get_layer('block3_pool').output\n",
    "    f4=vgg.get_layer('block4_pool').output\n",
    "    f5=vgg.get_layer('block5_pool').output\n",
    "#     f1=vgg.get_layer('block1_pool')\n",
    "#     f2=vgg.get_layer('block2_pool')\n",
    "#     f3=vgg.get_layer('block3_pool')\n",
    "#     f4=vgg.get_layer('block4_pool')\n",
    "#     f5=vgg.get_layer('block5_pool')\n",
    "    return (vgg.input,(f1,f2,f3,f4,f5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c991f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fcn_32(convs):\n",
    "    f1,f2,f3,f4,f5=convs\n",
    "    o = tf.keras.layers.Conv2DTranspose(12 , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(f5)\n",
    "    o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
    "    o = tf.keras.layers.Conv2DTranspose(12 , kernel_size=(16,16) ,  strides=(16,16) , use_bias=False )(o)\n",
    "\n",
    "    # append a softmax to get the class probabilities\n",
    "    o = (tf.keras.layers.Activation('softmax'))(o)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f006a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input=tf.keras.layers.Input((224,224,3))\n",
    "    input,convs=get_vgg_features(input)\n",
    "    output=get_fcn_32(convs)\n",
    "    model=tf.keras.Model(inputs=input,outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf6d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel=get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d9e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 12)        98304     \n",
      "_________________________________________________________________\n",
      "cropping2d (Cropping2D)      (None, 14, 14, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 224, 224, 12)      36864     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 224, 224, 12)      0         \n",
      "=================================================================\n",
      "Total params: 14,849,856\n",
      "Trainable params: 14,849,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "mymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "224e7587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "#import seaborn as sns\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0830c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['sky', 'building','column/pole', 'road', 'side walk', 'vegetation', 'traffic light', 'fence', 'vehicle', 'pedestrian', 'byciclist', 'void']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f433dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_filename_to_image_and_mask(t_filename, a_filename, height=224, width=224):\n",
    "  '''\n",
    "  Preprocesses the dataset by:\n",
    "    * resizing the input image and label maps\n",
    "    * normalizing the input image pixels\n",
    "    * reshaping the label maps from (height, width, 1) to (height, width, 12)\n",
    "\n",
    "  Args:\n",
    "    t_filename (string) -- path to the raw input image\n",
    "    a_filename (string) -- path to the raw annotation (label map) file\n",
    "    height (int) -- height in pixels to resize to\n",
    "    width (int) -- width in pixels to resize to\n",
    "\n",
    "  Returns:\n",
    "    image (tensor) -- preprocessed image\n",
    "    annotation (tensor) -- preprocessed annotation\n",
    "  '''\n",
    "\n",
    "  # Convert image and mask files to tensors \n",
    "  img_raw = tf.io.read_file(t_filename)\n",
    "  anno_raw = tf.io.read_file(a_filename)\n",
    "  image = tf.image.decode_jpeg(img_raw)\n",
    "  annotation = tf.image.decode_jpeg(anno_raw)\n",
    " \n",
    "  # Resize image and segmentation mask\n",
    "  image = tf.image.resize(image, (height, width,))\n",
    "  annotation = tf.image.resize(annotation, (height, width,))\n",
    "  image = tf.reshape(image, (height, width, 3,))\n",
    "  annotation = tf.cast(annotation, dtype=tf.int32)\n",
    "  annotation = tf.reshape(annotation, (height, width, 1,))\n",
    "  stack_list = []\n",
    "\n",
    "  # Reshape segmentation masks\n",
    "  for c in range(len(class_names)):\n",
    "    mask = tf.equal(annotation[:,:,0], tf.constant(c))\n",
    "    stack_list.append(tf.cast(mask, dtype=tf.int32))\n",
    "  \n",
    "  annotation = tf.stack(stack_list, axis=2)\n",
    "\n",
    "  # Normalize pixels in the input image\n",
    "  image = image/127.5\n",
    "  image -= 1\n",
    "\n",
    "  return image, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91658f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for preparing the datasets\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "def get_dataset_slice_paths(image_dir, label_map_dir):\n",
    "  '''\n",
    "  generates the lists of image and label map paths\n",
    "  \n",
    "  Args:\n",
    "    image_dir (string) -- path to the input images directory\n",
    "    label_map_dir (string) -- path to the label map directory\n",
    "\n",
    "  Returns:\n",
    "    image_paths (list of strings) -- paths to each image file\n",
    "    label_map_paths (list of strings) -- paths to each label map\n",
    "  '''\n",
    "  image_file_list = os.listdir(image_dir)\n",
    "  label_map_file_list = os.listdir(label_map_dir)\n",
    "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
    "  label_map_paths = [os.path.join(label_map_dir, fname) for fname in label_map_file_list]\n",
    "\n",
    "  return image_paths, label_map_paths\n",
    "\n",
    "\n",
    "def get_training_dataset(image_paths, label_map_paths):\n",
    "  '''\n",
    "  Prepares shuffled batches of the training set.\n",
    "  \n",
    "  Args:\n",
    "    image_paths (list of strings) -- paths to each image file in the train set\n",
    "    label_map_paths (list of strings) -- paths to each label map in the train set\n",
    "\n",
    "  Returns:\n",
    "    tf Dataset containing the preprocessed train set\n",
    "  '''\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
    "  training_dataset = training_dataset.map(map_filename_to_image_and_mask)\n",
    "  training_dataset = training_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
    "  training_dataset = training_dataset.batch(BATCH_SIZE)\n",
    "  training_dataset = training_dataset.repeat()\n",
    "  training_dataset = training_dataset.prefetch(-1)\n",
    "\n",
    "  return training_dataset\n",
    "\n",
    "\n",
    "def get_validation_dataset(image_paths, label_map_paths):\n",
    "  '''\n",
    "  Prepares batches of the validation set.\n",
    "  \n",
    "  Args:\n",
    "    image_paths (list of strings) -- paths to each image file in the val set\n",
    "    label_map_paths (list of strings) -- paths to each label map in the val set\n",
    "\n",
    "  Returns:\n",
    "    tf Dataset containing the preprocessed validation set\n",
    "  '''\n",
    "  validation_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
    "  validation_dataset = validation_dataset.map(map_filename_to_image_and_mask)\n",
    "  validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "  validation_dataset = validation_dataset.repeat()  \n",
    "\n",
    "  return validation_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f110d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths to the images\n",
    "training_image_paths, training_label_map_paths = get_dataset_slice_paths('fcnn-dataset/dataset1/images_prepped_train/','fcnn-dataset/dataset1/annotations_prepped_train/')\n",
    "validation_image_paths, validation_label_map_paths = get_dataset_slice_paths('fcnn-dataset/dataset1/images_prepped_test/','fcnn-dataset/dataset1/annotations_prepped_test/')\n",
    "\n",
    "# generate the train and val sets\n",
    "training_dataset = get_training_dataset(training_image_paths, training_label_map_paths)\n",
    "validation_dataset = get_validation_dataset(validation_image_paths, validation_label_map_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79526f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AbhijitShingote\\.conda\\envs\\tf2.6_092021\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "sgd = tf.keras.optimizers.SGD(lr=1E-2, momentum=0.9, nesterov=True)\n",
    "\n",
    "mymodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4546a9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0c6a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/170\n",
      "30/30 [==============================] - 26s 473ms/step - loss: 2.4850 - accuracy: 0.0818 - val_loss: 2.4846 - val_accuracy: 0.0872\n",
      "Epoch 2/170\n",
      "30/30 [==============================] - 18s 449ms/step - loss: 2.4838 - accuracy: 0.0918 - val_loss: 2.4830 - val_accuracy: 0.0989\n",
      "Epoch 3/170\n",
      "30/30 [==============================] - 13s 446ms/step - loss: 2.4809 - accuracy: 0.1079 - val_loss: 2.4784 - val_accuracy: 0.1174\n",
      "Epoch 4/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 2.4642 - accuracy: 0.1372 - val_loss: 2.4183 - val_accuracy: 0.1576\n",
      "Epoch 5/170\n",
      "30/30 [==============================] - 14s 452ms/step - loss: 2.1852 - accuracy: 0.2670 - val_loss: 1.9153 - val_accuracy: 0.2910\n",
      "Epoch 6/170\n",
      "30/30 [==============================] - 14s 454ms/step - loss: 1.7257 - accuracy: 0.3203 - val_loss: 1.7419 - val_accuracy: 0.2971\n",
      "Epoch 7/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 1.5386 - accuracy: 0.4056 - val_loss: 1.4780 - val_accuracy: 0.5002\n",
      "Epoch 8/170\n",
      "30/30 [==============================] - 14s 451ms/step - loss: 1.2660 - accuracy: 0.5692 - val_loss: 1.2996 - val_accuracy: 0.5553\n",
      "Epoch 9/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 1.0438 - accuracy: 0.6724 - val_loss: 1.1122 - val_accuracy: 0.6120\n",
      "Epoch 10/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.9583 - accuracy: 0.6865 - val_loss: 1.0367 - val_accuracy: 0.6216\n",
      "Epoch 11/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.9204 - accuracy: 0.6941 - val_loss: 0.9886 - val_accuracy: 0.6703\n",
      "Epoch 12/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.8448 - accuracy: 0.7343 - val_loss: 0.8576 - val_accuracy: 0.7545\n",
      "Epoch 13/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.7897 - accuracy: 0.7686 - val_loss: 0.8009 - val_accuracy: 0.7716\n",
      "Epoch 14/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.9519 - accuracy: 0.7161 - val_loss: 1.2120 - val_accuracy: 0.5532\n",
      "Epoch 15/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.8793 - accuracy: 0.7354 - val_loss: 0.8344 - val_accuracy: 0.7370\n",
      "Epoch 16/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.7716 - accuracy: 0.7751 - val_loss: 0.7894 - val_accuracy: 0.7570\n",
      "Epoch 17/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.7157 - accuracy: 0.7899 - val_loss: 0.7346 - val_accuracy: 0.7836\n",
      "Epoch 18/170\n",
      "30/30 [==============================] - 13s 446ms/step - loss: 0.6968 - accuracy: 0.7961 - val_loss: 0.7011 - val_accuracy: 0.7965\n",
      "Epoch 19/170\n",
      "30/30 [==============================] - 13s 445ms/step - loss: 0.6527 - accuracy: 0.8109 - val_loss: 0.6723 - val_accuracy: 0.8125\n",
      "Epoch 20/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.6446 - accuracy: 0.8137 - val_loss: 0.6656 - val_accuracy: 0.8136\n",
      "Epoch 21/170\n",
      "30/30 [==============================] - 14s 453ms/step - loss: 0.6228 - accuracy: 0.8200 - val_loss: 0.6344 - val_accuracy: 0.8212\n",
      "Epoch 22/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.6087 - accuracy: 0.8242 - val_loss: 0.6247 - val_accuracy: 0.8278\n",
      "Epoch 23/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.5871 - accuracy: 0.8314 - val_loss: 0.6018 - val_accuracy: 0.8293\n",
      "Epoch 24/170\n",
      "30/30 [==============================] - 13s 445ms/step - loss: 0.6154 - accuracy: 0.8212 - val_loss: 0.6407 - val_accuracy: 0.8176\n",
      "Epoch 25/170\n",
      "30/30 [==============================] - 13s 446ms/step - loss: 0.5870 - accuracy: 0.8307 - val_loss: 0.5858 - val_accuracy: 0.8321\n",
      "Epoch 26/170\n",
      "30/30 [==============================] - 13s 451ms/step - loss: 0.5595 - accuracy: 0.8376 - val_loss: 0.5810 - val_accuracy: 0.8319\n",
      "Epoch 27/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.5511 - accuracy: 0.8414 - val_loss: 0.5791 - val_accuracy: 0.8324\n",
      "Epoch 28/170\n",
      "30/30 [==============================] - 14s 455ms/step - loss: 0.5446 - accuracy: 0.8428 - val_loss: 0.5700 - val_accuracy: 0.8362\n",
      "Epoch 29/170\n",
      "30/30 [==============================] - 14s 458ms/step - loss: 0.5353 - accuracy: 0.8456 - val_loss: 0.5565 - val_accuracy: 0.8391\n",
      "Epoch 30/170\n",
      "30/30 [==============================] - 14s 458ms/step - loss: 0.5226 - accuracy: 0.8502 - val_loss: 0.5641 - val_accuracy: 0.8346\n",
      "Epoch 31/170\n",
      "30/30 [==============================] - 14s 457ms/step - loss: 0.5110 - accuracy: 0.8536 - val_loss: 0.5674 - val_accuracy: 0.8343\n",
      "Epoch 32/170\n",
      "30/30 [==============================] - 14s 459ms/step - loss: 0.5629 - accuracy: 0.8370 - val_loss: 0.5478 - val_accuracy: 0.8393\n",
      "Epoch 33/170\n",
      "30/30 [==============================] - 14s 457ms/step - loss: 0.5070 - accuracy: 0.8547 - val_loss: 0.5401 - val_accuracy: 0.8430\n",
      "Epoch 34/170\n",
      "30/30 [==============================] - 14s 455ms/step - loss: 0.4996 - accuracy: 0.8571 - val_loss: 0.5391 - val_accuracy: 0.8415\n",
      "Epoch 35/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.4885 - accuracy: 0.8603 - val_loss: 0.5316 - val_accuracy: 0.8437\n",
      "Epoch 36/170\n",
      "30/30 [==============================] - 13s 451ms/step - loss: 0.4849 - accuracy: 0.8618 - val_loss: 0.5281 - val_accuracy: 0.8447\n",
      "Epoch 37/170\n",
      "30/30 [==============================] - 14s 452ms/step - loss: 0.4779 - accuracy: 0.8632 - val_loss: 0.5245 - val_accuracy: 0.8472\n",
      "Epoch 38/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.4722 - accuracy: 0.8651 - val_loss: 0.5258 - val_accuracy: 0.8453\n",
      "Epoch 39/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.4715 - accuracy: 0.8657 - val_loss: 0.5329 - val_accuracy: 0.8446\n",
      "Epoch 40/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.4627 - accuracy: 0.8688 - val_loss: 0.5199 - val_accuracy: 0.8482\n",
      "Epoch 41/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.4599 - accuracy: 0.8691 - val_loss: 0.5189 - val_accuracy: 0.8492\n",
      "Epoch 42/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.4513 - accuracy: 0.8716 - val_loss: 0.5098 - val_accuracy: 0.8516\n",
      "Epoch 43/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.4411 - accuracy: 0.8751 - val_loss: 0.5055 - val_accuracy: 0.8547\n",
      "Epoch 44/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.4647 - accuracy: 0.8673 - val_loss: 0.5215 - val_accuracy: 0.8496\n",
      "Epoch 45/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.4357 - accuracy: 0.8763 - val_loss: 0.5110 - val_accuracy: 0.8524\n",
      "Epoch 46/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.4382 - accuracy: 0.8759 - val_loss: 0.5107 - val_accuracy: 0.8543\n",
      "Epoch 47/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.4264 - accuracy: 0.8789 - val_loss: 0.5005 - val_accuracy: 0.8532\n",
      "Epoch 48/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.4178 - accuracy: 0.8821 - val_loss: 0.4975 - val_accuracy: 0.8558\n",
      "Epoch 49/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.4224 - accuracy: 0.8802 - val_loss: 0.5100 - val_accuracy: 0.8539\n",
      "Epoch 50/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.4325 - accuracy: 0.8774 - val_loss: 0.5007 - val_accuracy: 0.8547\n",
      "Epoch 51/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.4128 - accuracy: 0.8829 - val_loss: 0.5010 - val_accuracy: 0.8597\n",
      "Epoch 52/170\n",
      "30/30 [==============================] - 14s 451ms/step - loss: 0.4042 - accuracy: 0.8859 - val_loss: 0.5059 - val_accuracy: 0.8575\n",
      "Epoch 53/170\n",
      "30/30 [==============================] - 13s 451ms/step - loss: 0.4113 - accuracy: 0.8834 - val_loss: 0.5087 - val_accuracy: 0.8589\n",
      "Epoch 54/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.4032 - accuracy: 0.8860 - val_loss: 0.4982 - val_accuracy: 0.8562\n",
      "Epoch 55/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.3938 - accuracy: 0.8891 - val_loss: 0.4954 - val_accuracy: 0.8569\n",
      "Epoch 56/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.3967 - accuracy: 0.8885 - val_loss: 0.4833 - val_accuracy: 0.8602\n",
      "Epoch 57/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 13s 448ms/step - loss: 0.3927 - accuracy: 0.8894 - val_loss: 0.4904 - val_accuracy: 0.8605\n",
      "Epoch 58/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.3880 - accuracy: 0.8906 - val_loss: 0.4839 - val_accuracy: 0.8602\n",
      "Epoch 59/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.3881 - accuracy: 0.8904 - val_loss: 0.4927 - val_accuracy: 0.8577\n",
      "Epoch 60/170\n",
      "30/30 [==============================] - 14s 452ms/step - loss: 0.3858 - accuracy: 0.8917 - val_loss: 0.4868 - val_accuracy: 0.8594\n",
      "Epoch 61/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.3781 - accuracy: 0.8939 - val_loss: 0.4952 - val_accuracy: 0.8582\n",
      "Epoch 62/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.3800 - accuracy: 0.8928 - val_loss: 0.4923 - val_accuracy: 0.8605\n",
      "Epoch 63/170\n",
      "30/30 [==============================] - 14s 453ms/step - loss: 0.3781 - accuracy: 0.8936 - val_loss: 0.4896 - val_accuracy: 0.8630\n",
      "Epoch 64/170\n",
      "30/30 [==============================] - 13s 452ms/step - loss: 0.3808 - accuracy: 0.8925 - val_loss: 0.4973 - val_accuracy: 0.8615\n",
      "Epoch 65/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.3762 - accuracy: 0.8938 - val_loss: 0.4834 - val_accuracy: 0.8632\n",
      "Epoch 66/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.3759 - accuracy: 0.8938 - val_loss: 0.4803 - val_accuracy: 0.8643\n",
      "Epoch 67/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.3655 - accuracy: 0.8971 - val_loss: 0.5158 - val_accuracy: 0.8558\n",
      "Epoch 68/170\n",
      "30/30 [==============================] - 14s 454ms/step - loss: 0.3731 - accuracy: 0.8945 - val_loss: 0.4952 - val_accuracy: 0.8587\n",
      "Epoch 69/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.3677 - accuracy: 0.8965 - val_loss: 0.4856 - val_accuracy: 0.8624\n",
      "Epoch 70/170\n",
      "30/30 [==============================] - 14s 452ms/step - loss: 0.3598 - accuracy: 0.8989 - val_loss: 0.4917 - val_accuracy: 0.8611\n",
      "Epoch 71/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.3608 - accuracy: 0.8985 - val_loss: 0.4925 - val_accuracy: 0.8615\n",
      "Epoch 72/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.3642 - accuracy: 0.8970 - val_loss: 0.4862 - val_accuracy: 0.8635\n",
      "Epoch 73/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.3543 - accuracy: 0.9005 - val_loss: 0.4897 - val_accuracy: 0.8609\n",
      "Epoch 74/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.3479 - accuracy: 0.9018 - val_loss: 0.4958 - val_accuracy: 0.8596\n",
      "Epoch 75/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.3737 - accuracy: 0.8948 - val_loss: 0.4896 - val_accuracy: 0.8598\n",
      "Epoch 76/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.3791 - accuracy: 0.8926 - val_loss: 0.4931 - val_accuracy: 0.8585\n",
      "Epoch 77/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.3486 - accuracy: 0.9018 - val_loss: 0.4924 - val_accuracy: 0.8595\n",
      "Epoch 78/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.3445 - accuracy: 0.9030 - val_loss: 0.4856 - val_accuracy: 0.8644\n",
      "Epoch 79/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.3469 - accuracy: 0.9025 - val_loss: 0.4946 - val_accuracy: 0.8617\n",
      "Epoch 80/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.3357 - accuracy: 0.9059 - val_loss: 0.4970 - val_accuracy: 0.8612\n",
      "Epoch 81/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.3338 - accuracy: 0.9063 - val_loss: 0.4960 - val_accuracy: 0.8621\n",
      "Epoch 82/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.3303 - accuracy: 0.9075 - val_loss: 0.4981 - val_accuracy: 0.8635\n",
      "Epoch 83/170\n",
      "30/30 [==============================] - 13s 447ms/step - loss: 0.3294 - accuracy: 0.9074 - val_loss: 0.5040 - val_accuracy: 0.8603\n",
      "Epoch 84/170\n",
      "30/30 [==============================] - 14s 455ms/step - loss: 0.3361 - accuracy: 0.9056 - val_loss: 0.5043 - val_accuracy: 0.8616\n",
      "Epoch 85/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.3320 - accuracy: 0.9067 - val_loss: 0.5086 - val_accuracy: 0.8601\n",
      "Epoch 86/170\n",
      "30/30 [==============================] - 14s 453ms/step - loss: 0.3246 - accuracy: 0.9087 - val_loss: 0.5056 - val_accuracy: 0.8633\n",
      "Epoch 87/170\n",
      "30/30 [==============================] - 13s 448ms/step - loss: 0.3342 - accuracy: 0.9059 - val_loss: 0.5028 - val_accuracy: 0.8632\n",
      "Epoch 88/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.3221 - accuracy: 0.9095 - val_loss: 0.5012 - val_accuracy: 0.8631\n",
      "Epoch 89/170\n",
      "30/30 [==============================] - 14s 452ms/step - loss: 0.3253 - accuracy: 0.9088 - val_loss: 0.5154 - val_accuracy: 0.8611\n",
      "Epoch 90/170\n",
      "30/30 [==============================] - 13s 449ms/step - loss: 0.3196 - accuracy: 0.9097 - val_loss: 0.5463 - val_accuracy: 0.8482\n",
      "Epoch 91/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.3307 - accuracy: 0.9065 - val_loss: 0.5214 - val_accuracy: 0.8604\n",
      "Epoch 92/170\n",
      "30/30 [==============================] - 14s 454ms/step - loss: 0.3170 - accuracy: 0.9111 - val_loss: 0.5106 - val_accuracy: 0.8630\n",
      "Epoch 93/170\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.3170 - accuracy: 0.9111 - val_loss: 0.5023 - val_accuracy: 0.8640\n",
      "Epoch 94/170\n",
      "30/30 [==============================] - 14s 456ms/step - loss: 0.3186 - accuracy: 0.9103 - val_loss: 0.5073 - val_accuracy: 0.8640\n",
      "Epoch 95/170\n",
      "30/30 [==============================] - 14s 460ms/step - loss: 0.3164 - accuracy: 0.9113 - val_loss: 0.5120 - val_accuracy: 0.8593\n",
      "Epoch 96/170\n",
      "30/30 [==============================] - 14s 460ms/step - loss: 0.3162 - accuracy: 0.9112 - val_loss: 0.5090 - val_accuracy: 0.8645\n",
      "Epoch 97/170\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.3124 - accuracy: 0.9126"
     ]
    }
   ],
   "source": [
    "# number of training images\n",
    "train_count = 367\n",
    "\n",
    "# number of validation images\n",
    "validation_count = 101\n",
    "\n",
    "EPOCHS = 170\n",
    "\n",
    "steps_per_epoch = train_count//BATCH_SIZE\n",
    "validation_steps = validation_count//BATCH_SIZE\n",
    "\n",
    "history = mymodel.fit(training_dataset,\n",
    "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_segments_test_arrays():\n",
    "  '''\n",
    "  Gets a subsample of the val set as your test set\n",
    "\n",
    "  Returns:\n",
    "    Test set containing ground truth images and label maps\n",
    "  '''\n",
    "  y_true_segments = []\n",
    "  y_true_images = []\n",
    "  test_count = 64\n",
    "\n",
    "  ds = validation_dataset.unbatch()\n",
    "  ds = ds.batch(101)\n",
    "\n",
    "  for image, annotation in ds.take(1):\n",
    "    y_true_images = image\n",
    "    y_true_segments = annotation\n",
    "\n",
    "\n",
    "  y_true_segments = y_true_segments[:test_count, : ,: , :]\n",
    "  y_true_segments = np.argmax(y_true_segments, axis=3)  \n",
    "\n",
    "  return y_true_images, y_true_segments\n",
    "\n",
    "# load the ground truth images and segmentation masks\n",
    "y_true_images, y_true_segments = get_images_and_segments_test_arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105dc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model prediction\n",
    "results = model.predict(validation_dataset, steps=validation_steps)\n",
    "\n",
    "# for each pixel, get the slice number which has the highest probability\n",
    "results = np.argmax(results, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "  '''\n",
    "  Computes IOU and Dice Score.\n",
    "\n",
    "  Args:\n",
    "    y_true (tensor) - ground truth label map\n",
    "    y_pred (tensor) - predicted label map\n",
    "  '''\n",
    "  \n",
    "  class_wise_iou = []\n",
    "  class_wise_dice_score = []\n",
    "\n",
    "  smoothening_factor = 0.00001\n",
    "\n",
    "  for i in range(12):\n",
    "    intersection = np.sum((y_pred == i) * (y_true == i))\n",
    "    y_true_area = np.sum((y_true == i))\n",
    "    y_pred_area = np.sum((y_pred == i))\n",
    "    combined_area = y_true_area + y_pred_area\n",
    "    \n",
    "    iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n",
    "    class_wise_iou.append(iou)\n",
    "    \n",
    "    dice_score =  2 * ((intersection + smoothening_factor) / (combined_area + smoothening_factor))\n",
    "    class_wise_dice_score.append(dice_score)\n",
    "\n",
    "  return class_wise_iou, class_wise_dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef54317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input a number from 0 to 63 to pick an image from the test set\n",
    "integer_slider = 0\n",
    "\n",
    "# compute metrics\n",
    "iou, dice_score = compute_metrics(y_true_segments[integer_slider], results[integer_slider])  \n",
    "\n",
    "# visualize the output and metrics\n",
    "show_predictions(y_true_images[integer_slider], [results[integer_slider], y_true_segments[integer_slider]], [\"Image\", \"Predicted Mask\", \"True Mask\"], iou, dice_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class-wise metrics\n",
    "cls_wise_iou, cls_wise_dice_score = compute_metrics(y_true_segments, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b72bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print IOU for each class\n",
    "for idx, iou in enumerate(cls_wise_iou):\n",
    "  spaces = ' ' * (13-len(class_names[idx]) + 2)\n",
    "  print(\"{}{}{} \".format(class_names[idx], spaces, iou)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the dice score for each class\n",
    "for idx, dice_score in enumerate(cls_wise_dice_score):\n",
    "  spaces = ' ' * (13-len(class_names[idx]) + 2)\n",
    "  print(\"{}{}{} \".format(class_names[idx], spaces, dice_score)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
